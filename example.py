#!/usr/bin/env python3
"""
å¤§æ¨¡å‹APIå‹åŠ›æµ‹è¯•ç³»ç»Ÿç¤ºä¾‹
"""

import asyncio
import time
from src.api.config import ConfigManager, PromptManager
from src.api.client import APIRequestManager
from src.engine.load_test import LoadTestEngine
from src.monitor.network import NetworkMonitor
from src.stats.analyzer import TestDatabase, DataAnalyzer
from src.report.generator import ReportManager


async def run_example_test():
    """è¿è¡Œç¤ºä¾‹æµ‹è¯•"""

    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨çš„æ˜¯ç¤ºä¾‹APIå¯†é’¥ï¼Œå®é™…ä½¿ç”¨æ—¶è¯·æ›¿æ¢ä¸ºçœŸå®çš„APIå¯†é’¥
    # æœ¬ç¤ºä¾‹ä¸ä¼šå®é™…å‘é€è¯·æ±‚ï¼Œä»…ç”¨äºæ¼”ç¤ºç³»ç»Ÿæ¶æ„

    print("ğŸš€ å¤§æ¨¡å‹APIå‹åŠ›æµ‹è¯•ç³»ç»Ÿç¤ºä¾‹")
    print("=" * 50)

    try:
        # 1. åˆå§‹åŒ–é…ç½®
        print("ğŸ“‹ åŠ è½½é…ç½®...")
        config_manager = ConfigManager()
        prompt_manager = PromptManager()

        # è·å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„APIé…ç½®
        api_names = config_manager.list_api_configs()
        if not api_names:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°APIé…ç½®")
            return

        api_name = api_names[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªAPIé…ç½®
        api_config = config_manager.get_api_config(api_name)

        # è·å–é»˜è®¤æµ‹è¯•é…ç½®
        test_config = config_manager.get_test_config('default')

        print(f"âœ… ä½¿ç”¨APIé…ç½®: {api_config.name}")
        print(f"âœ… ä½¿ç”¨æµ‹è¯•é…ç½®: {test_config.name}")
        print(f"âœ… åŠ è½½äº† {prompt_manager.get_prompt_count()} ä¸ªæµ‹è¯•æç¤ºè¯")

        # 2. æ¨¡æ‹Ÿæµ‹è¯•ï¼ˆä¸å‘é€çœŸå®è¯·æ±‚ï¼‰
        print("\nğŸ§ª å¼€å§‹æ¨¡æ‹Ÿæµ‹è¯•...")
        session_id = f"example_test_{int(time.time())}"

        # æ¨¡æ‹Ÿä¸€äº›æµ‹è¯•ç»“æœæ•°æ®
        from src.engine.load_test import LoadTestResult, RequestResult

        mock_results = []
        for concurrent_level in [1, 5, 10]:
            mock_request_results = []
            for i in range(20):  # æ¯ä¸ªå¹¶å‘çº§åˆ«20ä¸ªè¯·æ±‚
                mock_request_results.append(RequestResult(
                    timestamp=time.time() - (20 - i),
                    prompt=prompt_manager.get_next_prompt(),
                    response_time=0.5 + (concurrent_level * 0.1) + (i * 0.01),
                    status_code=200,
                    success=True,
                    response_content="Mock response",
                    error_message=None,
                    input_tokens=50,
                    output_tokens=100,
                    total_tokens=150,
                    content_length=200
                ))

            # åˆ†ææ¨¡æ‹Ÿç»“æœ
            engine = LoadTestEngine(None)  # ä¸éœ€è¦çœŸå®çš„request_managerç”¨äºåˆ†æ
            load_result = engine.analyze_results(mock_request_results, concurrent_level)
            mock_results.append(load_result)

            print(f"  ğŸ“Š å¹¶å‘ {concurrent_level}: å¹³å‡å“åº”æ—¶é—´ {load_result.avg_response_time:.2f}s, "
                  f"RPS {load_result.requests_per_second:.1f}")

        # 3. ä¿å­˜æµ‹è¯•ç»“æœ
        print("\nğŸ’¾ ä¿å­˜æµ‹è¯•ç»“æœ...")
        database = TestDatabase()

        start_time = time.time() - 300  # 5åˆ†é’Ÿå‰å¼€å§‹
        end_time = time.time()

        database.save_test_session(
            session_id=session_id,
            api_name=api_name,
            test_config='default',
            start_time=start_time,
            end_time=end_time,
            load_results=mock_results,
            metadata={'example': True}
        )

        print(f"âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜ï¼Œä¼šè¯ID: {session_id}")

        # 4. ç”Ÿæˆåˆ†ææŠ¥å‘Š
        print("\nğŸ“ˆ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        analyzer = DataAnalyzer(database)
        report_manager = ReportManager(analyzer)

        generated_reports = report_manager.generate_comprehensive_report(
            session_id=session_id,
            formats=['html', 'excel']
        )

        print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ:")
        for format_type, path in generated_reports.items():
            print(f"  ğŸ“„ {format_type.upper()}: {path}")

        # 5. æ˜¾ç¤ºæ‘˜è¦ç»Ÿè®¡
        print("\nğŸ“Š æµ‹è¯•æ‘˜è¦:")
        summary_stats = analyzer.generate_summary_stats(session_id)
        perf_metrics = summary_stats.get('performance_metrics', {})

        print(f"  æ€»è¯·æ±‚æ•°: {perf_metrics.get('total_requests', 0)}")
        print(f"  æˆåŠŸç‡: {perf_metrics.get('success_rate', 0) * 100:.1f}%")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {perf_metrics.get('avg_response_time', 0):.2f}s")
        print(f"  æ€»Tokenæ•°: {perf_metrics.get('total_tokens', 0)}")

        # 6. å¹¶å‘æ€§èƒ½åˆ†æ
        print("\nğŸ¯ å¹¶å‘æ€§èƒ½åˆ†æ:")
        concurrency_analysis = analyzer.analyze_concurrency_impact(session_id)
        if 'optimal_concurrency' in concurrency_analysis:
            optimal = concurrency_analysis['optimal_concurrency']
            print(f"  æ¨èå¹¶å‘æ•°: {optimal['concurrent_level']}")
            print(f"  é¢„æœŸRPS: {optimal['requests_per_second']:.1f}")
            print(f"  é¢„æœŸå“åº”æ—¶é—´: {optimal['avg_response_time']:.2f}s")

        print(f"\nğŸ‰ ç¤ºä¾‹æµ‹è¯•å®Œæˆï¼æŠ¥å‘Šå·²ç”Ÿæˆåˆ° reports/ ç›®å½•")
        print(f"ğŸ’¡ æç¤ºï¼šä½¿ç”¨ 'python main.py --help' æŸ¥çœ‹å®Œæ•´çš„å‘½ä»¤è¡Œé€‰é¡¹")

    except Exception as e:
        print(f"âŒ ç¤ºä¾‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(run_example_test())